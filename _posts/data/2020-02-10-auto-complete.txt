you might remember back in the old days when autocomplete was just terrible. the suggestions provided by autocomplete would be useless if not downright stupid i remember that one day when i intended to type "gimme a sec," only to see my message get edited into "gimme a sex" by the divine touches of autocomplete. on the same day, the feature was turned off on my phone for the betterment of the world. now, times have changed. recently, i decided to give autocorrect a chance on my iphone. surprisingly, i find myself liking autocomplete more than hating it, especially now that the weather is getting colder by each day: when my frost numbed finger tips touch on the wrong places of the phone screen to produce words that aren't really words, iphone's autocomplete somehow magically reads my mind to rearrange all that inscrutable alphabet soup into words that make actual, coherent sense. sometimes, it's so good at correcting my typos that i intentionnally make careless mistakes on the keyboard just to see how far it can go. one of the obvious reasons behind such drastic improvements in autocomplete functionality is the development of deep neural networks. as we know, neural networks are great at learning hidden patterns as long as we feed it with enough data. in this post, we will implement a very simple version of a generative deep neural network that can easily form the backbone of some character based autocomplete algorithm. let's begin! let's first go ahead and import all dependencies for this tutorial. as always, we will be using the functional api to build our neural network. we will be training our neural network to speak like the great german philosopher friedrich nietzsche . first, let's build a function that retrieves the necessary text file document from the web to return a python string. let's take a look at the text data by examining its length. character length: 600893 just to make sure that the data has been loaded successfully, let's take a look at the first 100 characters of the string. preface supposing that truth is a woman what then? is there not ground for suspecting that all ph it's time to preprocess the text data to make it feedable to our neural network. as introduced in this previous post on recurrent neural networks, the smart way to deal with text preprocessing is typically to use an embedding layer that translates words into vectors. however, text embedding is insuitable for this task since our goal is to build a character level text generation model. in other words, our model is not going to generate word predictions; instead, it will spit out a character each prediction cycle. therefore, we will use an alternative technique, namely mapping each character to an integer value. this isn't as elegant as text embedding or even one hot encoding but for a character level analysis, it should work fine. the function takes a string text data as input and returns a list of training data, each of length , sampled every characters. it also returns the training labels and a hash table mapping characters to their respective integer encodings. let's perform a quick sanity check to see if the function works as expected. specifying to 60 means that each instance in the training data will be 60 consecutive characters sampled from the text data every characters. number of sequences: 200278 number of unique characters: 57 the result tells us that we have a total of 200278 training instances, which is probably plenty to train, test, and validate our model. the result also tells us that there are 57 unique characters in the text data. note that these unique characters not only include alphabets but also and other miscellaneous white spacing characters and punctuations. let's now design our model. because there is obviously going to be sequential, temporal structure underlying the training data, we will use an lstm layer, a type of advanced recurrent neural network we saw in the previous post. in fact, this is all we need, unless we want to create a deep neural network spanning multiple layers. however, training such a model would cost a lot of time and computational resource. for the sake of simplicity, we will build a simple model with a single lstm layer. the output layer is going to be a dense layer with number of neurons, activated with a softmax function. we can thus interpret the index of the biggest value of the final array to correspond to the most likely character. below is a full plot of the model that shows the dimensions of the input and output tensors of all layers. now, all we have to do is to train the model with the data. let's run this for 50 epochs, just to give our model enough time to explore the loss function and settle on a good minimum. train on 200278 samples epoch 1/50 200278/200278 ============================== 164s 817us/sample loss: 2.5568 epoch 2/50 200278/200278 ============================== 163s 813us/sample loss: 2.1656 epoch 3/50 200278/200278 ============================== 162s 810us/sample loss: 2.0227 epoch 4/50 200278/200278 ============================== 162s 809us/sample loss: 1.9278 epoch 5/50 200278/200278 ============================== 161s 805us/sample loss: 1.8586 epoch 6/50 200278/200278 ============================== 162s 811us/sample loss: 1.8032 epoch 7/50 200278/200278 ============================== 163s 815us/sample loss: 1.7582 epoch 8/50 200278/200278 ============================== 165s 825us/sample loss: 1.7197 epoch 9/50 200278/200278 ============================== 167s 833us/sample loss: 1.6866 epoch 10/50 200278/200278 ============================== 166s 830us/sample loss: 1.6577 epoch 11/50 200278/200278 ============================== 165s 823us/sample loss: 1.6312 epoch 12/50 200278/200278 ============================== 162s 810us/sample loss: 1.6074 epoch 13/50 200278/200278 ============================== 162s 811us/sample loss: 1.5862 epoch 14/50 200278/200278 ============================== 161s 805us/sample loss: 1.5668 epoch 15/50 200278/200278 ============================== 165s 822us/sample loss: 1.5492 epoch 16/50 200278/200278 ============================== 166s 829us/sample loss: 1.5333 epoch 17/50 200278/200278 ============================== 167s 832us/sample loss: 1.5182 epoch 18/50 200278/200278 ============================== 166s 828us/sample loss: 1.5051 epoch 19/50 200278/200278 ============================== 166s 827us/sample loss: 1.4922 epoch 20/50 200278/200278 ============================== 164s 819us/sample loss: 1.4801 epoch 21/50 200278/200278 ============================== 165s 826us/sample loss: 1.4688 epoch 22/50 200278/200278 ============================== 165s 826us/sample loss: 1.4582 epoch 23/50 200278/200278 ============================== 165s 822us/sample loss: 1.4488 epoch 24/50 200278/200278 ============================== 163s 813us/sample loss: 1.4386 epoch 25/50 200278/200278 ============================== 167s 832us/sample loss: 1.4305 epoch 26/50 200278/200278 ============================== 166s 830us/sample loss: 1.4220 epoch 27/50 200278/200278 ============================== 167s 832us/sample loss: 1.4137 epoch 28/50 200278/200278 ============================== 167s 833us/sample loss: 1.4060 epoch 29/50 200278/200278 ============================== 166s 827us/sample loss: 1.3989 epoch 30/50 200278/200278 ============================== 164s 820us/sample loss: 1.3910 epoch 31/50 200278/200278 ============================== 163s 815us/sample loss: 1.3846 epoch 32/50 200278/200278 ============================== 162s 810us/sample loss: 1.3777 epoch 33/50 200278/200278 ============================== 162s 809us/sample loss: 1.3720 epoch 34/50 200278/200278 ============================== 160s 798us/sample loss: 1.3649 epoch 35/50 200278/200278 ============================== 163s 815us/sample loss: 1.3599 epoch 36/50 200278/200278 ============================== 162s 807us/sample loss: 1.3538 epoch 37/50 200278/200278 ============================== 162s 808us/sample loss: 1.3482 epoch 38/50 200278/200278 ============================== 162s 809us/sample loss: 1.3423 epoch 39/50 200278/200278 ============================== 163s 813us/sample loss: 1.3371 epoch 40/50 200278/200278 ============================== 164s 820us/sample loss: 1.3319 epoch 41/50 200278/200278 ============================== 163s 814us/sample loss: 1.3268 epoch 42/50 200278/200278 ============================== 165s 825us/sample loss: 1.3223 epoch 43/50 200278/200278 ============================== 164s 820us/sample loss: 1.3171 epoch 44/50 200278/200278 ============================== 164s 819us/sample loss: 1.3127 epoch 45/50 200278/200278 ============================== 165s 822us/sample loss: 1.3080 epoch 46/50 200278/200278 ============================== 163s 813us/sample loss: 1.3034 epoch 47/50 200278/200278 ============================== 163s 813us/sample loss: 1.2987 epoch 48/50 200278/200278 ============================== 162s 809us/sample loss: 1.2955 epoch 49/50 200278/200278 ============================== 161s 804us/sample loss: 1.2905 epoch 50/50 200278/200278 ============================== 162s 811us/sample loss: 1.2865 as i was training this model on google colab, i noticed that training even this simple model took a lot of time. therefore, i decided that it is a good idea to probably save the trained model in the worst case scenario that poor network connection suddenly caused the jupyter kernel to die, saving a saved model file would be of huge help since i can continue training again from there. saving the model on google colab requires us to import a simple module, . the process is very simple. to load the model, we can simply call the command below. let's take a look at the loss curve of the model. we can simply look at the value of the loss function as printed throughout the training scheme, but why not visualize it if we can? as expected, the loss decreases throughout each epoch. the reason i was not paticularly worried about overfitting was that we had so much data to work with, especially in comparison with the relatively constrained memory capacity of our one layered model. one of the objectives of this tutorial was to demonstrate the fun we can have with generative models, namely neural networks that can be used to generate data themselves, not just classify or predict data points. to put this into perspective, let's compare the objectives of a generative model with that of a discriminative model. simply put, the goal of a discriminative model is to model and calculate where is a label and is some input vector. as you can see, discriminative models arise most commonly from the context of supervised machine learning, such as regression or classification. in contrast, the goal of a generative model is to approximate the distribution which we might construe to be the probability of observing evidence or data. by modeling this distribution, the goal is that we might be able to generate samples that appear to have been sampled from this distribution. in other words, we want our model to generate likely data points based on an approximation of the true distribution from which these observations came from. in the context of this tutorial, our neural network should be able to somewhat immitate the speech of the famous german philosopher based on the training it went through with text data, although we would not expect the content generated by our neural network to have the same level of depth and profoundity as those of his original writings. as mentioned above, the objective of a generative model is to model the distribution of the latent space from which observed data points came from. at this point, our trained model should be able to model this distribution, and thus generate predictions given some input vector. ... god is dead god is dead god is dead... we don't want this to happen. instead, we want to introduce some noise so that the model faces subtle obstructions, thereby making it get more "creative" with its output instead of getting trapped in an infinite loop of some likely sequence. below is a sample implementation of adding noise to the output using log and exponential transformations to the output vector of our model. the transformation might be expressed as follows: where denotes a transformation, denotes a prediction as a vector, denotes temperature as a measure of randomness, and is a normalizing constant. although this might appear complicated, all it's doing is that it is adding some perturbation or disturbance to the output data so that it is possible for less likely characters to be chosen as the final prediction. below is a sample implementation of this process in code. note that due to the algebraic quality of the vector transformation above, randomness is increased for large values of . now it's finally time to put our nietzsche model to the test. how we will do this is pretty simple. first, we will feed a 60 character excerpt from the text to our model. then, the model will output a prediction vector, which is then passed onto given a specified . we will finally have a prediction that is 1 character. then, we incorporate that one character prediction into the original 60 character data we started with. we slice the new augmented data set from to end up with another prediction. we would then slice the data set from, you guessed it, and repeat the process as outlined above. when we iterate through this cycle many times, we would eventually end up with some generated text. below is the function that implements the iteration process. we're almost done! to get a better sense of what impact temperature has on the generation of text, let's quickly write up a function that will allow us to generate text for differing values of . the time has come: let's test our model for four different temperature values from 0.3 to 1.2, evenly spaced. we will make our model go through 1000 iterations to make sure that we have a long enough text to read, analyze, and evaluate. for the sake of readability, i have reformatted the output result in markdown quotations. generated text at temperature 0.3: is a woman what then? is there not ground for suspecting that the experience and present strange of the soul is also as the stand of the most profound that the present the art and possible to the present spore as a man and the morality and present self instinct, and the subject that the presence of the surcessize, and also it is an action which the philosophers and the spirit has the consider the action to the philosopher and possess and the spirit is not be who can something the predicess of the constinate the same and self interpatence, the disconsises what is not to be more profound, as if it is a man as a distance of the same art and ther strict to the presing to the result the problem of the present the spirit what is the consequences and the development of the same art of philosophers and security and spirit and for the subjective in the disturce, as in the contrary and present stronger and present could not be an inclination and desires of the same and distinguished that is the discoverty in such a person itself influence and ethers as generated text at temperature 0.6: is a woman what then? is there not ground for suspecting to and the world will had to a such that the basis of the incussions of the spirit as the does not because actian free spirits of intellect of the commstical purtious expression of men are so much he is not unnor experiences of self conturity, and as anifegently religious in the man would not consciously, his action is not be actian at in accombs life for the such all procees of great and the heart of this conduct the spirity of the man can provate for in any once in any of the suriticular conduct that which own needs, when they are therefore, as such action and some difficulty that the strength, it, himself which has to its fine term of pricismans the exacte in its self recuphing and every strength and man to wist the action something man as the worst, that the was of a longent that the whole not be all the very subjectical proves the stronger extent he is necessary to metaphysical figure of the faith in the bolity in the pure belief as "the such a successes of the values that is he â€‹ generated text at temperature 0.9: is a woman what then? is there not ground for suspecting that they grasutes, and so farmeduition of the does not only with this constrbicapity have honour and who distical seclles are denie'n, is one samiles are no luttrainess, and ethic and matficulty, concudes of morality to rost were presence of lighters caseful has prescally here at last not and servicatity, leads falled for child real appreparetess of worths the resticians when one to persans as a what a mean of that is as to the same heart tending noble stimptically and particious, we pach yought for that mankind, that the same take frights a contrady has howevers of a surplurating or in fact a sort, without present superite fimatical matterm of our being interlunally men who cal scornce. the shrinking's proglish, and traints he way to demitable pure explised and place can deterely by the compulse in whom is phypociative cinceous, and the higher and will bounthen in itsiluariant upon find the "first the whore we man will simple condection and some than us a valuasly refiges who feel generated text at temperature 1.2: is a woman what then? is there not ground for suspecting that he therefore when shre, mun, a schopenhehtor abold gevert. 120 =as in find that is _know believinally bad, euser of view. bithic iftel canly in any knowitumentially. the charm surpose again, in swret feathryst, form of kinne of the world bejud age implaasoun ever? but that the is any appearance has clenge: the? a plexable gen preducl=s than condugebleines and aligh to advirenta nasure; findiminal it as, not take. the ideved towards upavanizing, would be thenion, in all pespres: it is of a concidenary, which, well founly con utbacte udwerlly upon mansing frauble of "arrey been can the pritarnated from their christian often think prestation of mocives." legt, lenge: this deps telows, plenhance of decessaticrances). hyrk an interlusally" tone under good haggy," is have we leamness of conschous should it, of sicking ummenfeckinal zerturm erienweron of noble of himself clonizing there is conctumendable prefersy exaitunia states," whether they deve oves any of hispyssesss. int the results are fascinating. granted, our model is still bad at immitating nietzsche's style of writing, but i think the performance is impressive given that this was a character based text generation model. think about it for a second: to write even a single word, say "present," the model has to correctly predict "p", "r", "e", "s", "e", "n", and "t," all in tandem. imagine doing this for extended cycles, long enough to generate text that is comfortably a paragraph long. it's amazing how the text it generates even makes some sense at all. then, as temperature rises, we see more randomness and "creativity" at work. we start to see more words that aren't really words . at temperature 1.2, the model is basically going crazy with randomness, adding white spaces where there shouldn't be and sounding more and more like a speaker of old english or german, something that one might expect to see in english scripts written in pre shakesperean times. at any rate, it is simply fascinating to see how a neural network can be trained to immitate some style of writing. hopefully this tutorial gave you some intuition of how autocomplete works, although i presume business grade autocomplete functions on our phones are based on much more complicated algorithms. thanks for reading this post. in the next post, we might look at another example of a generative model known as generative adversarial networks, or gan for short. this is a burgeoning field in deep learning with a lot of prospect and attention, so i'm already excited to put out that post once it's done. see you in the next post. peace!